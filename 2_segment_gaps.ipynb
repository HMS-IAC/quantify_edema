{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 option A: Applying an existing pixelclassifier \n",
    "We have trained a pixelclassifier to segment gaps between cells --- labelled as \"fluid\". \n",
    "Download the classifier from (!!) and place it (!!)\n",
    "The following script applies the classifier to an image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluid = getAnnotationObjects().findAll{it.getPathClass() == getPathClass(\"fluid\")}\n",
    "\n",
    "if (!fluid) {\n",
    "    selectObjectsByClassification(\"tissue\")\n",
    "    createAnnotationsFromPixelClassifier(\"edema_20241006\", 5.0, 15.0)\n",
    "    fireHierarchyUpdate()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. option B: Training and applying a pixelclassifier from scratch  \n",
    "The following script samples non-overlapping square regions of interest within all regions marked \"tissue\" \n",
    "Modify the value for sidelength to make the boxes bigger or smaller\n",
    "Modify the value for n_regions to change the number of regions of interest to sample. \n",
    "\n",
    "Run for all images you want to include in training\n",
    "\n",
    "The regions are named \"Other\" to distinguish them from training annotations for the tissue training image. \n",
    "\n",
    "Run Classify ‣ Training images ‣ Create training image \n",
    "\n",
    "and select \"Other\" in the drop-down menu under Classification. Then click \"ok\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qupath.lib.roi.RectangleROI\n",
    "import qupath.lib.objects.PathAnnotationObject\n",
    "import qupath.lib.images.servers.ImageServer\n",
    "\n",
    "double sidelength = 500 // side-length of square ROI in um\n",
    "int n_regions = 10 // number of regions to generate\n",
    "\n",
    "int seed = -1\n",
    "def rng = new Random()\n",
    "if (seed >= 0)\n",
    "    rng.setSeed(seed)\n",
    "\n",
    "\n",
    "\n",
    "selectObjectsByClassification(\"tissue\");\n",
    "def selected = getSelectedObject()  \n",
    "\n",
    "\n",
    "tissueAnnotation = getAnnotationObjects().find{it.getPathClass() == getPathClass(\"tissue\")}\n",
    "tissueroi = tissueAnnotation.getROI()\n",
    "println(tissueroi.getClass())\n",
    "\n",
    "def imageData=getCurrentImageData()\n",
    "pixelwidth = imageData.getServer().getPixelCalibration().getPixelWidth()\n",
    "\n",
    "println(pixelwidth)\n",
    "pxsidelength = sidelength/pixelwidth\n",
    "\n",
    "\n",
    "int count = 0\n",
    "double x = 0\n",
    "double y = 0\n",
    "\n",
    "def xs = [];\n",
    "def ys = [];\n",
    "\n",
    "while (count < n_regions) {\n",
    "    println count\n",
    "    if (Thread.currentThread().isInterrupted()) {\n",
    "        println 'Interrupted!'\n",
    "        return\n",
    "    }\n",
    "    x = tissueroi.getBoundsX() + rng.nextDouble() * tissueroi.getBoundsWidth()\n",
    "    y = tissueroi.getBoundsY() + rng.nextDouble() * tissueroi.getBoundsHeight()\n",
    "\n",
    "    if (!tissueroi.contains(x + 0.5*pxsidelength, y + 0.5*pxsidelength)) //if the centre of the box is contained in tissueroi, incresase count\n",
    "        continue\n",
    "        \n",
    "    // discard a box if it overlaps with an already existing box \n",
    "    def grx = xs.collect{ it - pxsidelength < x }\n",
    "    def smlx = xs.collect{ it + pxsidelength > x }\n",
    "    def gry = ys.collect{ it - pxsidelength < y }\n",
    "    def smly = ys.collect{ it + pxsidelength > y }\n",
    "    def istrue = [grx, smlx, gry, smly].transpose().collect {it.every{ it }}\n",
    "    \n",
    "    if (istrue.any {it})\n",
    "        continue\n",
    "    \n",
    "    xs.add(x);\n",
    "    ys.add(y);\n",
    "    count++\n",
    "    \n",
    "    // generate ROIs for boxes that passed all conditions\n",
    "    roi = ROIs.createRectangleROI(x, y, pxsidelength, pxsidelength, ImagePlane.getDefaultPlane())\n",
    "    pathObject = PathObjects.createAnnotationObject(roi, getPathClass(\"Region_gap*\") )\n",
    "    addObject(pathObject) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General tips when generating training annotations  \n",
    "Stay away from the edges of each patch.  \n",
    "Make annotations small and roughly equal size -- Many small annotations will better represent the diversity of your data than few large annotations.  \n",
    "Annotations should not overlap  \n",
    "\n",
    "1. Open a newly generated composite image\n",
    "2. On each patch derived from each training image, draw a small region that represents the class \"gap\". Skip a patch if it has no gap in it. \n",
    "3. All thus-far made selections therefore belong to the class \"gap\". Mark all of them in the Annotations tab (click \"Select all\". or shift+click on the first and last annotation. Or cmd+click (ctrl+click on Windows) on each annotation belonging to \"gap\")\n",
    "4. With all relevant annotations highlighted, click on the corresponding class in the classification list to the right a click \"set selected\". All marked annotations should now be assigned to the class \"gap\". \n",
    "5. Repeat steps 2-4, but this time mark regions that look similar to gaps, but aren't. Asign these to the class \"Ignore*\". Be careful not to wrongly re-assign annotations of class \"gap\" to ignore (and vice versa)\n",
    "6.  \n",
    "** Open Classify > Pixel classification > Train Pixel classifier. Follow [the documentation to train your pixel classifier](https://qupath.readthedocs.io/en/stable/docs/tutorials/pixel_classification.html#getting-started). In short:\n",
    "** Below are example settings for the pixel classifier: Only values deviating from standard settings are shown. I recommend experimenting with these settings.  \n",
    "** Next, name your classifier: enter a name in the Classifier name field. Click Save.  \n",
    "** Click on Live prediction. This may take some time to load  \n",
    "** The accuaracy of the trained pixel classifier can be viewed in the Log. click shift+cmd(ctrl)+L to open the log. Look for a message that starts with \"Current accuracy on the TRAINING SET...\"  \n",
    "** To toggle the prediction on and off, press C  \n",
    "** You can toggle Output from Classification to Probability. Classification shows binary classification, whereas Probability colorcodes the confidence of the prediction.  \n",
    "Once you are happy with the filters, turn off live prediction by clicking on \"Live prediction\". Now it is time to correct wrongly predicted pixels. When output is \"Probability\", you can search for the most confidently incorrect prediction per patch.  \n",
    "Thereby repeat point 2-4, except that instead of drawing new training regions, you mark regions that were wrongly classified.  \n",
    "** Click on \"Live prediction\" again to re-compute the classification.  \n",
    "** Once happy, save your classifier  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **Classifier**: Random trees (RTrees)\n",
    "* **Resolution**: Very high (0.5 um/px)\n",
    "* **Features** > Edit: \n",
    "    - **Channels**: \n",
    "    Hematoxylin,\n",
    "    Eosin, \n",
    "    Residual\n",
    "    - **Scales**:\n",
    "    1.0, \n",
    "    4.0, \n",
    "    8.0 \n",
    "    - **Features**: \n",
    "    Gaussian, \n",
    "    Gradient magnitude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the classifier to all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "├── classifiers  \n",
    "│   ├── classes.json  \n",
    "│   └── pixel_classifiers  \n",
    "│       ├── edema_20241006.json  \n",
    "│       ├── gaps_tutorial.json  \n",
    "│       └── tissue_background.json  \n",
    "├── data  \n",
    "│   └── ...  \n",
    "├── project.qpproj  \n",
    "├── project.qpproj.backup  \n",
    "├── scripts  \n",
    "│   ├── segment_gaps.groovy  \n",
    "│   ├── segment_tissue.groovy  \n",
    "│   ├── training_annotation.groovy  \n",
    "│   └── training_annotation_gaps.groovy  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# qupath_project\n",
    "\n",
    "- **classifiers**\n",
    "  - `classes.json`\n",
    "  - **pixel_classifiers**\n",
    "    - `edema_20241006.json`\n",
    "    - `gaps_tutorial.json`\n",
    "    - `tissue_background.json`\n",
    "- **data** \n",
    "- `project.qpproj`\n",
    "- `project.qpproj.backup`\n",
    "- **scripts**\n",
    "  - `segment_gaps.groovy`\n",
    "  - `segment_tissue.groovy`\n",
    "  - `training_annotation.groovy`\n",
    "  - `training_annotation_gaps.groovy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step may take a long time depending on your image size, compute capability, and classifier settings. Computation on a cluster may be necessary. \n",
    "Run it on a single image/small tissue region first.  \n",
    "\n",
    "Run the following script for one/all images. \n",
    "\n",
    "classifier_name: enter the name of the classifier you trained. \n",
    "\n",
    "In a QuPath project, \n",
    "\n",
    "project folder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = \"gaps_tutorial\" // Enter the name of the classifier you want to use\n",
    "min_area = 5.0 // Minimum area (um^2) of fragments to keep \n",
    "min_hole_size = 15.0 // minimum area of holes to keep (um^2)\n",
    "\n",
    "fluid = getAnnotationObjects().findAll{it.getPathClass() == getPathClass(\"fluid\")}\n",
    "\n",
    "\n",
    "if (!fluid) { // only run if fluid is not already annotated\n",
    "    selectObjectsByClassification(\"tissue\") // Only classify within the tissue annotation\n",
    "    //createAnnotationsFromPixelClassifier(\"edema_20241006\", min_area, min_hole_size)\n",
    "    createAnnotationsFromPixelClassifier(classifier_name, min_area, min_hole_size) \n",
    "    fireHierarchyUpdate()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Export segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following script in QuPath to export the gaps segmentation. \n",
    "name_tissue: write the name of the tissue segments here within quotation marks \n",
    "name_gaps: write the name of the gap segments within QuPath here, using quotation marks \n",
    "\n",
    "Images will be saved as tiff files. \n",
    "\n",
    "Folderstructure: \n",
    "# qupath_project\n",
    "\n",
    "- **classifiers**\n",
    "  - `classes.json`\n",
    "  - **pixel_classifiers**\n",
    "    - `edema_20241006.json`\n",
    "    - `gaps_tutorial.json`\n",
    "    - `tissue_background.json`\n",
    "- **data** \n",
    "- `project.qpproj`\n",
    "- `project.qpproj.backup`\n",
    "- **scripts**\n",
    "  - `segment_gaps.groovy`\n",
    "  - `segment_tissue.groovy`\n",
    "  - `training_annotation.groovy`\n",
    "  - `training_annotation_gaps.groovy`\n",
    "- **export** \n",
    "    - `name_image_1`\n",
    "      - `fluid.tif` \n",
    "      - `tissue.tif`\n",
    "    - `name_image_2`\n",
    "    - `name_image_n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qupath.lib.images.servers.LabeledImageServer\n",
    "\n",
    "name_tissue = 'tissue'\n",
    "name_gaps = 'fluid'\n",
    "\n",
    "\n",
    "\n",
    "fluid = getAnnotationObjects().findAll{it.getPathClass() == getPathClass(name_gaps)}\n",
    "\n",
    "\n",
    "if (fluid) {\n",
    "    \n",
    "    def imageData = getCurrentImageData()\n",
    "    \n",
    "    // Define output path (relative to project)\n",
    "    def name = GeneralTools.getNameWithoutExtension(imageData.getServer().getMetadata().getName())\n",
    "    def pathOutput = buildFilePath(PROJECT_BASE_DIR, 'export', name)\n",
    "    mkdirs(pathOutput)\n",
    "    \n",
    "    // Define output resolution\n",
    "    // double requestedPixelSize = 1.0\n",
    "    \n",
    "    // Convert to downsample. Factor of 1 means no downsampling. \n",
    "    double downsample = 1.0 //requestedPixelSize / imageData.getServer().getPixelCalibration().getAveragedPixelSize()\n",
    "    \n",
    "    // Create an ImageServer where the pixels are derived from annotations\n",
    "    def edemaServer = new LabeledImageServer.Builder(imageData)\n",
    "        .backgroundLabel(0, ColorTools.WHITE) // Specify background label (usually 0 or 255)\n",
    "        .downsample(downsample)    // Choose server resolution; this should match the resolution at which tiles are exported\n",
    "        .addLabel(name_gaps, 1)\n",
    "        .grayscale()\n",
    "        .multichannelOutput(false) // If true, each label refers to the channel of a multichannel binary image (required for multiclass probability)\n",
    "        .build()\n",
    "    \n",
    "    def tissueServer = new LabeledImageServer.Builder(imageData)\n",
    "        .backgroundLabel(0, ColorTools.WHITE) // Specify background label (usually 0 or 255)\n",
    "        .downsample(downsample)    // Choose server resolution; this should match the resolution at which tiles are exported\n",
    "        .addLabel(name_tissue, 1)      // Choose output labels (the order matters!)\n",
    "        .grayscale()\n",
    "        .multichannelOutput(false) // If true, each label refers to the channel of a multichannel binary image (required for multiclass probability)\n",
    "        .build()\n",
    "    \n",
    "    \n",
    "    // Export things within the tissue bounding box \n",
    "    annotation = getAnnotationObjects().find{it.getPathClass() == getPathClass(name_tissue)}\n",
    "    def edema_region = RegionRequest.createInstance(edemaServer.getPath(), downsample, annotation.getROI())\n",
    "    def tissue_region = RegionRequest.createInstance(tissueServer.getPath(), downsample, annotation.getROI())\n",
    "    def tissuePath = buildFilePath(pathOutput, name_tissue + '.tif')\n",
    "    def edemaPath = buildFilePath(pathOutput, name_gaps + '.tif')\n",
    "\n",
    "    \n",
    "    writeImageRegion(edemaServer, edema_region, edemaPath)\n",
    "    writeImageRegion(tissueServer, tissue_region, tissuePath)\n",
    "    \n",
    " }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
