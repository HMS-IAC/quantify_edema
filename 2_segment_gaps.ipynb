{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 option A: Applying an existing pixelclassifier \n",
    "We have trained a pixelclassifier (```edema_20241006```) to segment gaps between cells labelled as \"fluid\".  \n",
    "Download the classifier from the ```pixel_classifiers``` folder in GitHub and place it your QuPath project folder under ```classifiers/pixel_classifiers/```\n",
    "The following script applies the: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluid = getAnnotationObjects().findAll{it.getPathClass() == getPathClass(\"fluid\")}\n",
    "\n",
    "if (!fluid) {\n",
    "    selectObjectsByClassification(\"tissue\")\n",
    "    createAnnotationsFromPixelClassifier(\"edema_20241006\", 5.0, 15.0)\n",
    "    fireHierarchyUpdate()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. option B: Training and applying a pixelclassifier from scratch  \n",
    "The following script samples non-overlapping square regions of interest within all regions classed as \"tissue\"  \n",
    "Modify the value for ```sidelength``` to make the boxes bigger or smaller  \n",
    "Modify the value for ```n_regions``` to change the number of regions of interest to sample.  \n",
    "\n",
    "Run for all images you want to include in training.  \n",
    "\n",
    "The regions are named \"Other\" to distinguish them from training annotations for the tissue training image.  \n",
    "\n",
    "Run ```Classify ‣ Training images ‣ Create training image```  \n",
    "\n",
    "and select \"Other\" in the drop-down menu under Classification. Then click \"ok\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qupath.lib.roi.RectangleROI\n",
    "import qupath.lib.objects.PathAnnotationObject\n",
    "import qupath.lib.images.servers.ImageServer\n",
    "\n",
    "double sidelength = 500 // side-length of square ROI in um\n",
    "int n_regions = 10 // number of regions to generate\n",
    "\n",
    "int seed = -1\n",
    "def rng = new Random()\n",
    "if (seed >= 0)\n",
    "    rng.setSeed(seed)\n",
    "\n",
    "\n",
    "\n",
    "selectObjectsByClassification(\"tissue\");\n",
    "def selected = getSelectedObject()  \n",
    "\n",
    "\n",
    "tissueAnnotation = getAnnotationObjects().find{it.getPathClass() == getPathClass(\"tissue\")}\n",
    "tissueroi = tissueAnnotation.getROI()\n",
    "println(tissueroi.getClass())\n",
    "\n",
    "def imageData=getCurrentImageData()\n",
    "pixelwidth = imageData.getServer().getPixelCalibration().getPixelWidth()\n",
    "\n",
    "println(pixelwidth)\n",
    "pxsidelength = sidelength/pixelwidth\n",
    "\n",
    "\n",
    "int count = 0\n",
    "double x = 0\n",
    "double y = 0\n",
    "\n",
    "def xs = [];\n",
    "def ys = [];\n",
    "\n",
    "while (count < n_regions) {\n",
    "    println count\n",
    "    if (Thread.currentThread().isInterrupted()) {\n",
    "        println 'Interrupted!'\n",
    "        return\n",
    "    }\n",
    "    x = tissueroi.getBoundsX() + rng.nextDouble() * tissueroi.getBoundsWidth()\n",
    "    y = tissueroi.getBoundsY() + rng.nextDouble() * tissueroi.getBoundsHeight()\n",
    "\n",
    "    if (!tissueroi.contains(x + 0.5*pxsidelength, y + 0.5*pxsidelength)) //if the centre of the box is contained in tissueroi, incresase count\n",
    "        continue\n",
    "        \n",
    "    // discard a box if it overlaps with an already existing box \n",
    "    def grx = xs.collect{ it - pxsidelength < x }\n",
    "    def smlx = xs.collect{ it + pxsidelength > x }\n",
    "    def gry = ys.collect{ it - pxsidelength < y }\n",
    "    def smly = ys.collect{ it + pxsidelength > y }\n",
    "    def istrue = [grx, smlx, gry, smly].transpose().collect {it.every{ it }}\n",
    "    \n",
    "    if (istrue.any {it})\n",
    "        continue\n",
    "    \n",
    "    xs.add(x);\n",
    "    ys.add(y);\n",
    "    count++\n",
    "    \n",
    "    // generate ROIs for boxes that passed all conditions\n",
    "    roi = ROIs.createRectangleROI(x, y, pxsidelength, pxsidelength, ImagePlane.getDefaultPlane())\n",
    "    pathObject = PathObjects.createAnnotationObject(roi, getPathClass(\"Region_gap*\") )\n",
    "    addObject(pathObject) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: General tips when generating training annotations  \n",
    "> * Stay away from the edges of each patch.  \n",
    "> * Make annotations small and roughly equal size — Many small annotations will better represent the diversity of your data than few large annotations.  \n",
    "> * Annotations should not overlap  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step by step: \n",
    "1. Open the newly generated composite image\n",
    "2. On each patch derived from each training image, draw a small region that represents the class \"gap\". Skip a patch if it has no gap in it.  \n",
    "3. All thus-far made selections therefore belong to the class \"gap\". Mark all of them in the ```Annotations``` tab  \n",
    "(To do so, click \"Select all\". or shift+click on the first and last annotation. Or cmd+click (ctrl+click on Windows) on each annotation belonging to \"gap\")\n",
    "4. With all relevant annotations highlighted, click on the corresponding class in the classification list to the right a click \"set selected\". All marked annotations should now be assigned to the class \"gap\".  \n",
    "5. Repeat steps 2-4, but this time mark regions that are not gaps. Asign these to the class \"Ignore*\". Be careful not to wrongly re-assign annotations of class \"gap\" to ignore (and vice versa)\n",
    "\n",
    "6. Open Classify > Pixel classification > Train Pixel classifier. Follow [the documentation to train your pixel classifier](https://qupath.readthedocs.io/en/stable/docs/tutorials/pixel_classification.html#getting-started). In short:\n",
    "7. Below are example settings for the pixel classifier: Only values deviating from standard settings are shown. I recommend experimenting with these settings.  \n",
    "8. Name your classifier: enter a name in the Classifier name field. Click ```Save```.  \n",
    "9. Click on Live prediction. This may take some time to load  \n",
    "10. Inspect the result:  \n",
    "    * To toggle the prediction on and off, press C  \n",
    "    * You can toggle Output from Classification to Probability. Classification shows binary classification, whereas Probability colorcodes the confidence of the prediction.  \n",
    "    * The accuaracy of the trained pixel classifier can be viewed in the Log. click shift+cmd(ctrl)+L to open the log. Look for a message that starts with \"Current accuracy on the TRAINING SET...\" \n",
    "\n",
    "11. Once you are happy with the filters, turn off live prediction by clicking on \"Live prediction\".  \n",
    "12. Correct wrongly predicted pixels. When output is \"Probability\", you can search for the most confidently incorrect prediction per patch.  \n",
    "Repeat point 2-4, except marking regions that were wrongly classified.  \n",
    "13. Click on \"Live prediction\" again to re-compute the classification.  \n",
    "14. Once happy, save your classifier by clicking ```Save```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pixelclassifier with these settings can be found in ```pixel_classifiers``` in GitHub named ```gaps_tutorial.json```\n",
    "* **Classifier**: Random trees (RTrees)\n",
    "* **Resolution**: Very high (0.5 um/px)\n",
    "* **Features** > Edit: \n",
    "    - **Channels**: \n",
    "    Hematoxylin,\n",
    "    Eosin, \n",
    "    Residual\n",
    "    - **Scales**:\n",
    "    1.0, \n",
    "    4.0, \n",
    "    8.0 \n",
    "    - **Features**: \n",
    "    Gaussian, \n",
    "    Gradient magnitude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the classifier to all images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step may take a long time depending on your image size, compute capability, and classifier settings. Computation on a cluster may be necessary.  \n",
    "You can test it on a single image/small tissue region first.  \n",
    "\n",
    "Run the following script for one/all images. Modify these variables as necessary:  \n",
    "\n",
    "**classifier_name**: enter the name of the classifier you trained. The classifier should be in ```classifiers/pixel_classifiers```  \n",
    "**min_area**: The the minimum area of a region to keep.  Smaller fragments will be discarded.  \n",
    "**min_hole_size**: minimum hole area. Holes smaller than this area will be filled.  \n",
    "**name_gaps**: The name of the QuPath class for gaps between cells. For instance \"gaps\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = \"gaps_tutorial\" // Enter the name of the classifier you want to use\n",
    "min_area = 5.0 // Minimum area (um^2) of fragments to keep \n",
    "min_hole_size = 15.0 // minimum area of holes to keep (um^2)\n",
    "name_gaps = \"fluid\"\n",
    "\n",
    "fluid = getAnnotationObjects().findAll{it.getPathClass() == getPathClass(name_gaps)}\n",
    "\n",
    "\n",
    "if (!fluid) { // only run if fluid is not already annotated\n",
    "    selectObjectsByClassification(\"tissue\") // Only classify within the tissue annotation\n",
    "    //createAnnotationsFromPixelClassifier(\"edema_20241006\", min_area, min_hole_size)\n",
    "    createAnnotationsFromPixelClassifier(classifier_name, min_area, min_hole_size) \n",
    "    fireHierarchyUpdate()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Export segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Make a folder for images to be exported  \n",
    "First, make a folder inside your QuPath project folder named ```export```.  \n",
    "This is where the exported images will be saved. Your folder structure should look like this:  \n",
    "\n",
    "**qupath_project**\n",
    "- **classifiers**\n",
    "  - `classes.json`\n",
    "  - **pixel_classifiers**\n",
    "    - `edema_20241006.json`\n",
    "    - `gaps_tutorial.json`\n",
    "    - `tissue_background.json`\n",
    "- **data** \n",
    "- `project.qpproj`\n",
    "- `project.qpproj.backup`\n",
    "- **scripts**\n",
    "  - `segment_gaps.groovy`\n",
    "  - `segment_tissue.groovy`\n",
    "  - `training_annotation.groovy`\n",
    "  - `training_annotation_gaps.groovy`\n",
    "- **export** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Export segmentation from QuPath \n",
    "Run the following script in QuPath to export the gap segmentation.  \n",
    "**name_tissue**: write the name of the tissue segments here within quotation marks  \n",
    "**name_gaps**: write the name of the gap segments within QuPath here, using quotation marks  \n",
    "\n",
    "Images will be saved as tiff files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qupath.lib.images.servers.LabeledImageServer\n",
    "\n",
    "name_tissue = 'tissue'\n",
    "name_gaps = 'fluid'\n",
    "\n",
    "\n",
    "fluid = getAnnotationObjects().findAll{it.getPathClass() == getPathClass(name_gaps)}\n",
    "\n",
    "\n",
    "if (fluid) {\n",
    "    \n",
    "    def imageData = getCurrentImageData()\n",
    "    \n",
    "    // Define output path (relative to project)\n",
    "    def name = GeneralTools.getNameWithoutExtension(imageData.getServer().getMetadata().getName())\n",
    "    def pathOutput = buildFilePath(PROJECT_BASE_DIR, 'export', name)\n",
    "    mkdirs(pathOutput)\n",
    "    \n",
    "    // Define output resolution\n",
    "    // double requestedPixelSize = 1.0\n",
    "    \n",
    "    // Convert to downsample. Factor of 1 means no downsampling. \n",
    "    double downsample = 1.0 //requestedPixelSize / imageData.getServer().getPixelCalibration().getAveragedPixelSize()\n",
    "    \n",
    "    // Create an ImageServer where the pixels are derived from annotations\n",
    "    def edemaServer = new LabeledImageServer.Builder(imageData)\n",
    "        .backgroundLabel(0, ColorTools.WHITE) // Specify background label (usually 0 or 255)\n",
    "        .downsample(downsample)    // Choose server resolution; this should match the resolution at which tiles are exported\n",
    "        .addLabel(name_gaps, 1)\n",
    "        .grayscale()\n",
    "        .multichannelOutput(false) // If true, each label refers to the channel of a multichannel binary image (required for multiclass probability)\n",
    "        .build()\n",
    "    \n",
    "    def tissueServer = new LabeledImageServer.Builder(imageData)\n",
    "        .backgroundLabel(0, ColorTools.WHITE) // Specify background label (usually 0 or 255)\n",
    "        .downsample(downsample)    // Choose server resolution; this should match the resolution at which tiles are exported\n",
    "        .addLabel(name_tissue, 1)      // Choose output labels (the order matters!)\n",
    "        .grayscale()\n",
    "        .multichannelOutput(false) // If true, each label refers to the channel of a multichannel binary image (required for multiclass probability)\n",
    "        .build()\n",
    "    \n",
    "    \n",
    "    // Export things within the tissue bounding box \n",
    "    annotation = getAnnotationObjects().find{it.getPathClass() == getPathClass(name_tissue)}\n",
    "    def edema_region = RegionRequest.createInstance(edemaServer.getPath(), downsample, annotation.getROI())\n",
    "    def tissue_region = RegionRequest.createInstance(tissueServer.getPath(), downsample, annotation.getROI())\n",
    "    def tissuePath = buildFilePath(pathOutput, name_tissue + '.tif')\n",
    "    def edemaPath = buildFilePath(pathOutput, name_gaps + '.tif')\n",
    "\n",
    "    \n",
    "    writeImageRegion(edemaServer, edema_region, edemaPath)\n",
    "    writeImageRegion(tissueServer, tissue_region, tissuePath)\n",
    "    \n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the output \n",
    "The export folder (see above) should now have the following structure: \n",
    "* A folder for each image that was processed. In each of which:  \n",
    "* Two files: One containing the tissue mask (below as \"tissue.tif\"), the other gap segmentation mask (below as \"fluid.tif\")\n",
    "\n",
    "\n",
    "- **export** \n",
    "    - `name_image_1`\n",
    "      - `fluid.tif` \n",
    "      - `tissue.tif`\n",
    "    - `name_image_2`\n",
    "    - `name_image_n`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
